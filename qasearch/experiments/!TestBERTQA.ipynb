{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "color-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "desirable-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None \n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "\n",
    "config_switch=os.getenv('DOCKER', 'local')\n",
    "if config_switch=='local':\n",
    "    startup_nodes = [{\"host\": \"127.0.0.1\", \"port\": \"30001\"}, {\"host\": \"127.0.0.1\", \"port\":\"30002\"}, {\"host\":\"127.0.0.1\", \"port\":\"30003\"}]\n",
    "else:\n",
    "    startup_nodes = [{\"host\": \"rgcluster\", \"port\": \"30001\"}, {\"host\": \"rgcluster\", \"port\":\"30002\"}, {\"host\":\"rgcluster\", \"port\":\"30003\"}]\n",
    "\n",
    "try: \n",
    "    from redisai import ClusterClient\n",
    "    redisai_cluster_client = ClusterClient(startup_nodes=startup_nodes)\n",
    "except:\n",
    "    print(\"Redis Cluster is not available\")\n",
    "\n",
    "def loadTokeniser():\n",
    "    global tokenizer\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def qa_redisai(question, sentence_key,hash_tag):\n",
    "    ### question is encoded\n",
    "    ### use pre-computed context/answer text tensor\n",
    "\n",
    "    global tokenizer\n",
    "\n",
    "    if not tokenizer:\n",
    "        tokenizer=loadTokeniser()\n",
    "\n",
    "     \n",
    "\n",
    "    token_key = f\"tokenized:bert:qa:{sentence_key}\"\n",
    "\n",
    "    input_ids_question = tokenizer.encode(question, add_special_tokens=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "\n",
    "    \n",
    "    input_ids_context=redisai_cluster_client.tensorget(token_key)\n",
    "    input_ids = np.append(input_ids_question,input_ids_context)\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    print(input_ids)\n",
    "    attention_mask = np.array([[1]*len(input_ids)])\n",
    "    input_idss=np.array([input_ids])\n",
    "    print(input_idss.shape)\n",
    "    print(\"Attention mask shape \",attention_mask.shape)\n",
    "    \n",
    "    num_seg_a=input_ids_question.shape[1]\n",
    "    print(num_seg_a)\n",
    "    num_seg_b=input_ids_context.shape[0]\n",
    "    print(num_seg_b)\n",
    "    token_type_ids = np.array([0]*num_seg_a + [1]*num_seg_b)\n",
    "    print(\"Segments id\",token_type_ids.shape)\n",
    "    \n",
    "    redisai_cluster_client.tensorset(f'input_ids{hash_tag}', input_idss)\n",
    "    redisai_cluster_client.tensorset(f'attention_mask{hash_tag}', attention_mask)\n",
    "    redisai_cluster_client.tensorset(f'token_type_ids{hash_tag}', token_type_ids)\n",
    "\n",
    "    redisai_cluster_client.modelrun(f'bert-qa{hash_tag}', [f'input_ids{hash_tag}', f'attention_mask{hash_tag}', f'token_type_ids{hash_tag}'],\n",
    "                        [f'answer_start_scores{hash_tag}', f'answer_end_scores{hash_tag}'])\n",
    "    print(f\"Model run on {hash_tag}\")\n",
    "    answer_start_scores = redisai_cluster_client.tensorget(f'answer_start_scores{hash_tag}')\n",
    "    answer_end_scores = redisai_cluster_client.tensorget(f'answer_end_scores{hash_tag}')\n",
    "\n",
    "    answer_start = np.argmax(answer_start_scores)\n",
    "    answer_end = np.argmax(answer_end_scores) + 1\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end], skip_special_tokens = True))\n",
    "    print(answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "occasional-fitness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182,)\n",
      "[  101  2054  2055 13139  1997  5258  8625  5897 12987  1029   102  1056\n",
      "  1044  1041  1042  1054  1041  1053  1057  1041  1050  1039  1045  1041\n",
      "  1055  1051  1042  1051  1039  1039  1057  1054  1054  1041  1050  1039\n",
      "  1041  1042  1051  1054  1045  1050  1057  1039  1048  1041  1051  1056\n",
      "  1045  1040  1041  1055  1059  1041  1054  1041  1039  1051  1049  1052\n",
      "  1037  1054  1041  1040  1056  1051  1056  1044  1041  1054  1037  1050\n",
      "  1040  1051  1049  1054  1050  1037  1039  1051  1057  1050  1056  1041\n",
      "  1054  1052  1037  1054  1056  1055  1044  1037  1058  1045  1050  1043\n",
      "  1056  1044  1041  1055  1037  1049  1041  1038  1037  1055  1041  1052\n",
      "  1054  1051  1052  1051  1054  1056  1045  1051  1050  1045  1050  1051\n",
      "  1054  1040  1041  1054  1056  1051  1039  1051  1049  1052  1057  1056\n",
      "  1041  1056  1044  1041  1037  1058  1037  1048  1057  1041  1056  1044\n",
      "  1037  1056  1054  1041  1042  1048  1041  1039  1056  1041  1040  1056\n",
      "  1044  1041  1045  1054  1045  1050  1057  1039  1048  1041  1051  1056\n",
      "  1045  1040  1041  1038  1045  1037  1055  1056  1037  1038  1048  1041\n",
      "  1016   102]\n",
      "(1, 182)\n",
      "Attention mask shape  (1, 182)\n",
      "11\n",
      "171\n",
      "Segments id (182,)\n",
      "Model run on {06S}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What about frequencies of occurenence RNA?\"\n",
    "qa_redisai(question,\"PMC222961.xml:{06S}:26\",'{06S}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Effectiveness of community contact reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_key=\"PMC261870.xml:{06S}:26\"\n",
    "token_key = f\"tokenized:bert:qa:{sentence_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "redisai_cluster_client.connection_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "slot = redisai_cluster_client.connection_pool.nodes.keyslot(sentence_key)\n",
    "node = redisai_cluster_client.connection_pool.get_master_node_by_slot(slot)\n",
    "connection = redisai_cluster_client.connection_pool.get_connection_by_node(node)\n",
    "connection.send_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)\n",
    "print(connection.__dict__)\n",
    "print(redisai_cluster_client.parse_response(connection,\"RG.TRIGGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "slot = redisai_cluster_client.connection_pool.nodes.keyslot(sentence_key)\n",
    "node = redisai_cluster_client.connection_pool.get_master_node_by_slot(slot)\n",
    "connection = redisai_cluster_client.connection_pool.get_connection_by_node(node)\n",
    "connection.send_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)\n",
    "print(connection.__dict__)\n",
    "print(redisai_cluster_client.parse_response(connection,\"RG.TRIGGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rediscluster import RedisCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "startup_nodes = [{\"host\": \"127.0.0.1\", \"port\": \"30001\"}, {\"host\": \"127.0.0.1\", \"port\":\"30002\"}, {\"host\":\"127.0.0.1\", \"port\":\"30003\"}]\n",
    "rc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_methods = [method_name for method_name in dir(rc)\n",
    "                  if callable(getattr(rc, method_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_key=\"PMC261870.xml:{06S}:26\"\n",
    "question=\"Effectiveness of community contact reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.execute_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "command='RG.TRIGGER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.determine_node('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rc.nodes_flags.get(command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(args)>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.execute_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-costa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rediscluster import RedisCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from rediscluster import RedisCluster\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('rediscluster')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.execute_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.connection_pool.nodes.random_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rc.connection_pool.nodes.all_masters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.get(sentence_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rc.parse_response(connection,\"RG.TRIGGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=rc.get(\"cache{06S}_PMC261870.xml:{06S}:26_Effectiveness of community contact reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diagnostic-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None \n",
    "model = None\n",
    "\n",
    "import torch\n",
    "\n",
    "def loadTokeniser():\n",
    "    global tokenizer\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", torchscript=True)\n",
    "    return tokenizer\n",
    "\n",
    "def loadModel():\n",
    "    global model\n",
    "    from transformers import AutoModelForQuestionAnswering\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", torchscript=True)\n",
    "    return model\n",
    "\n",
    "def qa(question, content_text):\n",
    "    global tokenizer, model \n",
    "\n",
    "    if not tokenizer:\n",
    "        tokenizer=loadTokeniser()\n",
    "\n",
    "    if not model:\n",
    "        model=loadModel()\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, content_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    print(input_ids)\n",
    "\n",
    "    answer_start_scores, answer_end_scores = model(**inputs,return_dict=False)\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "christian-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text=\"The frequencies of occurrence for i nucleotides were compared to the random RNA counterparts having the same base proportion in order to compute the a value that reflected their i nucleotide bias Table 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacterial-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What about frequencies of occurenence RNA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "restricted-victorian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2055, 13139, 1997, 5258, 8625, 5897, 12987, 1029, 102, 1996, 13139, 1997, 14404, 2005, 1045, 16371, 14321, 26601, 2015, 2020, 4102, 2000, 1996, 6721, 12987, 14562, 2383, 1996, 2168, 2918, 10817, 1999, 2344, 2000, 24134, 1996, 1037, 3643, 2008, 7686, 2037, 1045, 16371, 14321, 26601, 13827, 2795, 1016, 102]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = None \n",
    "model = None\n",
    "\n",
    "import torch\n",
    "\n",
    "def loadTokeniser():\n",
    "    global tokenizer\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    return tokenizer\n",
    "\n",
    "def loadModel():\n",
    "    global model\n",
    "    from transformers import AutoModelForQuestionAnswering\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    return model\n",
    "\n",
    "if not tokenizer:\n",
    "    tokenizer=loadTokeniser()\n",
    "\n",
    "if not model:\n",
    "    model=loadModel()\n",
    "\n",
    "inputs = tokenizer.encode_plus(question, content_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "print(input_ids)\n",
    "\n",
    "answer_start_scores, answer_end_scores = model(**inputs,return_dict=False)\n",
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mathematical-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compared to the random rna counterparts having the same base proportion\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "waiting-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2055, 13139, 1997, 5258, 8625, 5897, 12987, 1029, 102, 1996, 13139, 1997, 14404, 2005, 1045, 16371, 14321, 26601, 2015, 2020, 4102, 2000, 1996, 6721, 12987, 14562, 2383, 1996, 2168, 2918, 10817, 1999, 2344, 2000, 24134, 1996, 1037, 3643, 2008, 7686, 2037, 1045, 16371, 14321, 26601, 13827, 2795, 1016, 102]\n",
      "compared to the random rna counterparts having the same base proportion\n",
      "CPU times: user 12.4 s, sys: 1.4 s, total: 13.8 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(qa(question,content_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "certified-retro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cache{06S}_PMC222961.xml:{06S}:26_What about frequencies of occurenence RNA?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cache{06S}_PMC222961.xml:{06S}:26_%s\" % question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "national-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"When air samples collected?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "champion-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cache{5M5}_PMC140314.xml:{5M5}:44_When air samples collected?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cache{5M5}_PMC140314.xml:{5M5}:44_%s\" % question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cordless-showcase",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-cee2556dec3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache{06S}_PMC261870.xml:{06S}:26_%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rc' is not defined"
     ]
    }
   ],
   "source": [
    "rc.get(\"cache{5M5}_PMC261870.xml:{5M5}:26_%s\" % question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "demonstrated-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(input_ids):\n",
    "    # BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "    # tokenizer's behavior, let's also get the token strings and display them.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # For each token and its id...\n",
    "    for token, id in zip(tokens, input_ids):\n",
    "\n",
    "        # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "        if id == tokenizer.sep_token_id:\n",
    "            print('')\n",
    "\n",
    "        # Print the token string and its ID in two columns.\n",
    "        print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "        if id == tokenizer.sep_token_id:\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "native-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    " \n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids]),return_dict=False) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cooperative-matrix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 51 tokens.\n",
      "\n",
      "Answer: \"compared to the random rna counterparts having the same base proportion\"\n"
     ]
    }
   ],
   "source": [
    "answer_question(question, content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hget sentence:PMC222961.xml:{06S} 26\n",
    "\"The frequencies of occurrence for i nucleotides were compared to the random RNA counterparts having the same base proportion in order to compute the a value that reflected their i nucleotide bias Table 2\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-macedonia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
